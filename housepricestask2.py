# -*- coding: utf-8 -*-
"""housepricesTask2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NJ7TthIZSfddt0CDvvKKqzyo_P0OEgEv
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.svm import SVR, SVC
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
import joblib

# Load the dataset
df = pd.read_csv('boston.csv', encoding='latin')

# Display initial data
print(df.head())
print(df.describe())
print(df.dtypes)

# 1. Handling Missing Values
df.fillna(df.median(), inplace=True)

# 2. Handling Duplicates
df = df.drop_duplicates()

# 3. Handling Categorical Data
categorical_cols = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# 4. Handling Outliers using IQR
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Creating a categorical target variable for classification
df['MEDV_category'] = pd.cut(df['MEDV'], bins=[-np.inf, 20, 30, np.inf], labels=['Low', 'Medium', 'High'])

# Splitting features and target variables for regression and classification
X = df.drop(['MEDV', 'MEDV_category'], axis=1)
y_reg = df['MEDV']  # Continuous target for regression
y_class = df['MEDV_category']  # Categorical target for classification

# 5. Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Splitting data into training and testing sets for regression and classification
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, y_reg, test_size=0.2, random_state=42)
X_train, X_test, y_train_class, y_test_class = train_test_split(X_scaled, y_class, test_size=0.2, random_state=42)

# Dictionary to store model performance
model_performance = {}

# --- REGRESSION MODELS ---

# Linear Regression (using y_train_reg for regression tasks)
lr = LinearRegression()
lr.fit(X_train_reg, y_train_reg)
y_pred_lr = lr.predict(X_test_reg)
model_performance['Linear Regression'] = {
    'R2 Score': r2_score(y_test_reg, y_pred_lr),
    'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_lr))
}

# Decision Tree Regressor (using y_train_reg for regression tasks)
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train_reg, y_train_reg)
y_pred_dt = dt.predict(X_test_reg)
model_performance['Decision Tree'] = {
    'R2 Score': r2_score(y_test_reg, y_pred_dt),
    'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_dt))
}

# Random Forest Regressor (using y_train_reg for regression tasks)
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train_reg, y_train_reg)
y_pred_rf = rf.predict(X_test_reg)
model_performance['Random Forest'] = {
    'R2 Score': r2_score(y_test_reg, y_pred_rf),
    'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_rf))
}

# Hyperparameter Tuning with Grid Search for Random Forest Regressor
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
}
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search_rf.fit(X_train_reg, y_train_reg)

# Best parameters and performance for the tuned Random Forest model
best_rf = grid_search_rf.best_estimator_
y_pred_best_rf = best_rf.predict(X_test_reg)
model_performance['Tuned Random Forest'] = {
    'R2 Score': r2_score(y_test_reg, y_pred_best_rf),
    'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_best_rf))
}

# --- CLASSIFICATION MODELS ---

# Logistic Regression (using y_train_class for classification tasks)
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train_class)
y_pred_log_reg = log_reg.predict(X_test)
model_performance['Logistic Regression'] = {
    'Accuracy': accuracy_score(y_test_class, y_pred_log_reg),
    'Classification Report': classification_report(y_test_class, y_pred_log_reg),
    'Confusion Matrix': confusion_matrix(y_test_class, y_pred_log_reg)
}

# Decision Tree Classifier (using y_train_class for classification tasks)
dtc = DecisionTreeClassifier(random_state=42)
dtc.fit(X_train, y_train_class)
y_pred_dtc = dtc.predict(X_test)
model_performance['Decision Tree Classifier'] = {
    'Accuracy': accuracy_score(y_test_class, y_pred_dtc),
    'Classification Report': classification_report(y_test_class, y_pred_dtc),
    'Confusion Matrix': confusion_matrix(y_test_class, y_pred_dtc)
}

# Random Forest Classifier (using y_train_class for classification tasks)
rfc = RandomForestClassifier(random_state=42)
rfc.fit(X_train, y_train_class)
y_pred_rfc = rfc.predict(X_test)
model_performance['Random Forest Classifier'] = {
    'Accuracy': accuracy_score(y_test_class, y_pred_rfc),
    'Classification Report': classification_report(y_test_class, y_pred_rfc),
    'Confusion Matrix': confusion_matrix(y_test_class, y_pred_rfc)
}

# Support Vector Classifier (using y_train_class for classification tasks)
svc = SVC()
svc.fit(X_train, y_train_class)
y_pred_svc = svc.predict(X_test)
model_performance['Support Vector Classifier'] = {
    'Accuracy': accuracy_score(y_test_class, y_pred_svc),
    'Classification Report': classification_report(y_test_class, y_pred_svc),
    'Confusion Matrix': confusion_matrix(y_test_class, y_pred_svc)
}

# K-Nearest Neighbors Classifier (using y_train_class for classification tasks)
knn = KNeighborsClassifier()
knn.fit(X_train, y_train_class)
y_pred_knn = knn.predict(X_test)
model_performance['K-Nearest Neighbors Classifier'] = {
    'Accuracy': accuracy_score(y_test_class, y_pred_knn),
    'Classification Report': classification_report(y_test_class, y_pred_knn),
    'Confusion Matrix': confusion_matrix(y_test_class, y_pred_knn)
}

# Model Performance Comparison
performance_df = pd.DataFrame(model_performance).T
print("Model Performance Comparison:\n", performance_df)

# Saving the Best Model (Random Forest Classifier after tuning)
best_model = best_rf
joblib.dump(best_model, 'best_model.pkl')
print("Best model saved as 'best_model.pkl'")